{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0bdb4a-a3b7-4b51-8451-fc1012f14625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome! This script will process images from a zip file using DBSCAN.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN # Using DBSCAN\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "\n",
    "# --- Image Processing Functions ---\n",
    "\n",
    "def load_and_preprocess_image(img_raw: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Converts a raw BGR image to grayscale, applies Gaussian blur, and then\n",
    "    performs histogram equalization (CLAHE).\n",
    "    Returns the original BGR, grayscale, and preprocessed grayscale images.\n",
    "    \"\"\"\n",
    "    if img_raw is None:\n",
    "        print(\"Error: Input image is None during load/preprocess.\")\n",
    "        return None, None, None\n",
    "\n",
    "    original_bgr = img_raw.copy()\n",
    "    gray = cv2.cvtColor(original_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Apply CLAHE from your K-Means example for consistency in preprocessing\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    equalized = clahe.apply(blurred) \n",
    "\n",
    "    return original_bgr, gray, equalized\n",
    "\n",
    "def segment_image_dbscan(image: np.ndarray, eps: float = 5, min_samples: int = 38) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Segments an image using DBSCAN clustering.\n",
    "    Returns:\n",
    "      - labels: (H*W,) array of cluster assignments per pixel (-1 for noise)\n",
    "      - segmented_image: (H, W) image where each pixel is colored based on its cluster,\n",
    "                         with the largest non-noise cluster highlighted (e.g., black)\n",
    "    \"\"\"\n",
    "    rows, cols = image.shape\n",
    "    \n",
    "    # Create feature vectors (row, col, intensity) for DBSCAN\n",
    "    features = []\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            features.append([r, c, image[r, c]]) # Use preprocessed intensity\n",
    "    features = np.array(features, dtype=np.float32)\n",
    "\n",
    "    # Apply DBSCAN clustering\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(features) # labels will contain -1 for noise\n",
    "\n",
    "    # Create the segmented image based on DBSCAN results\n",
    "    segmented_image = np.ones((rows, cols, 3), dtype=np.uint8) * 255 # Start with white background\n",
    "\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    # Identify the largest non-noise cluster to highlight (e.g., in black)\n",
    "    # This helps visualize the primary segmentation result.\n",
    "    non_noise_labels_indices = np.where(unique_labels != -1)[0]\n",
    "    \n",
    "    if len(non_noise_labels_indices) > 0:\n",
    "        # Get counts for non-noise labels\n",
    "        non_noise_counts = counts[non_noise_labels_indices]\n",
    "        # Find the label of the largest non-noise cluster\n",
    "        largest_cluster_label = unique_labels[non_noise_labels_indices[np.argmax(non_noise_counts)]]\n",
    "        \n",
    "        # Color the largest cluster black\n",
    "        mask = (labels.reshape(rows, cols) == largest_cluster_label)\n",
    "        segmented_image[mask] = [0, 0, 0] # Set to black (0,0,0)\n",
    "\n",
    "        # You could also color other non-noise clusters with different shades of gray/colors\n",
    "        # For simplicity, we highlight the largest cluster and leave others white/noise.\n",
    "        \n",
    "    return labels, segmented_image\n",
    "\n",
    "def blend_images(original_preprocessed_gray: np.ndarray, segmented_dbscan: np.ndarray, alpha: float = 0.7) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Blends the preprocessed grayscale image with the DBSCAN segmented image.\n",
    "    Note: DBSCAN segmented image here is expected to be BGR (0 for segmented, 255 for background)\n",
    "    \"\"\"\n",
    "    orig_f = original_preprocessed_gray.astype(np.float32) / 255.0 # Normalize to 0-1\n",
    "    \n",
    "    # Convert DBSCAN segmented image to a single channel (0-1 range) for blending\n",
    "    # Assuming black (0) for foreground and white (255) for background\n",
    "    segmented_f = cv2.cvtColor(segmented_dbscan, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0\n",
    "\n",
    "    # Invert the segmented image so that the black region (segmentation) becomes high intensity (1)\n",
    "    # and white (background) becomes low intensity (0). This makes blending more intuitive.\n",
    "    inverted_segmented_f = 1.0 - segmented_f\n",
    "\n",
    "    # Blend: Alpha determines how much of the inverted segmented image is used.\n",
    "    # Higher alpha means the segmented region will show up more prominently.\n",
    "    blended_f = (1 - alpha) * orig_f + alpha * inverted_segmented_f\n",
    "    \n",
    "    blended_uint8 = cv2.normalize(blended_f, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    return blended_uint8\n",
    "\n",
    "\n",
    "def enhance_edges(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies a sharpening kernel to enhance edges.\n",
    "    \"\"\"\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    return cv2.filter2D(image, -1, kernel)\n",
    "\n",
    "def compute_segmentation_metrics(\n",
    "    gt_mask: np.ndarray,\n",
    "    pred_labels: np.ndarray,\n",
    "    n_unique_pred_labels: int):\n",
    "    \"\"\"\n",
    "    Compute Accuracy, Normalized Mutual Information (NMI), and Adjusted Rand Index (ARI)\n",
    "    between a ground-truth mask and DBSCAN cluster labels.\n",
    "    \n",
    "    NOTE: For DBSCAN, the 'n_unique_pred_labels' should include the noise label (-1)\n",
    "    if you want it considered in the confusion matrix. If you want to evaluate only\n",
    "    the found clusters, you'd filter pred_labels to exclude -1.\n",
    "    For consistency with the K-Means approach, we'll try to map all unique labels.\n",
    "    \n",
    "    Args:\n",
    "      - gt_mask: 2D array of integer labels (shape: H×W). Each distinct integer is a separate label.\n",
    "      - pred_labels: 1D array of cluster assignments (length H*W), integers (can include -1 for noise).\n",
    "      - n_unique_pred_labels: total number of unique labels in pred_labels (including -1 if present).\n",
    "        \n",
    "    Returns:\n",
    "      (accuracy, nmi, ari)\n",
    "      - accuracy: float in [0,1], computed by finding the optimal one-to-one mapping\n",
    "                  between cluster IDs and true labels via the Hungarian algorithm.\n",
    "      - nmi: float in [0,1], normalized mutual information between gt and pred (ignoring label permutations).\n",
    "      - ari: float in [−1,1], adjusted Rand index between gt and pred.\n",
    "    \"\"\"\n",
    "    gt_flat = gt_mask.flatten()\n",
    "    pred = pred_labels.copy()\n",
    "\n",
    "    # Determine unique true labels and remap them to indices [0..n_true-1]\n",
    "    true_labels_unique = np.unique(gt_flat)\n",
    "    n_true = len(true_labels_unique)\n",
    "    label_to_index = {lab: idx for idx, lab in enumerate(true_labels_unique)}\n",
    "    gt_indices = np.vectorize(label_to_index.get)(gt_flat)  # shape (H*W,)\n",
    "\n",
    "    # Build confusion matrix: counts[i,j] = #pixels where pred==i and true==j\n",
    "    # Need to handle potential -1 in pred_labels by mapping them to an index\n",
    "    pred_labels_unique = np.unique(pred)\n",
    "    pred_label_to_matrix_idx = {lab: idx for idx, lab in enumerate(pred_labels_unique)}\n",
    "    pred_matrix_indices = np.vectorize(pred_label_to_matrix_idx.get)(pred)\n",
    "\n",
    "    counts = np.zeros((len(pred_labels_unique), n_true), dtype=np.int64)\n",
    "    for idx in range(pred.shape[0]):\n",
    "        counts[pred_matrix_indices[idx], gt_indices[idx]] += 1\n",
    "\n",
    "    # Use Hungarian algorithm to find optimal one-to-one assignment between clusters and true labels\n",
    "    # If the number of predicted labels is different from true labels, linear_sum_assignment handles this by padding.\n",
    "    cost_matrix = -counts\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "    # Build mapping: pred_label_value → true_label_value\n",
    "    # Use pred_labels_unique to map back to original DBSCAN labels (-1, 0, 1, ...)\n",
    "    mapping = {pred_labels_unique[cluster_matrix_idx]: true_labels_unique[true_idx] for cluster_matrix_idx, true_idx in zip(row_ind, col_ind)}\n",
    "    \n",
    "    # Map predicted labels using the optimal mapping\n",
    "    pred_mapped = np.array([mapping.get(label, -1) for label in pred]) # Default to -1 if label not mapped (e.g., unmapped noise)\n",
    "\n",
    "    # Compute Accuracy\n",
    "    # Filter out ground truth labels that don't have a corresponding cluster if necessary,\n",
    "    # or ensure gt_mask is consistent with desired comparison for accuracy.\n",
    "    # For now, we'll compare everything, including noise.\n",
    "    acc = accuracy_score(gt_flat, pred_mapped)\n",
    "\n",
    "    # Compute NMI and ARI on the original labels (they are permutation-invariant internally)\n",
    "    nmi = normalized_mutual_info_score(gt_flat, pred)\n",
    "    ari = adjusted_rand_score(gt_flat, pred)\n",
    "\n",
    "    return acc, nmi, ari\n",
    "\n",
    "def plot_metric_barchart(metrics_dict: dict, title: str = 'Segmentation Metrics'):\n",
    "    \"\"\"\n",
    "    Given a dictionary with keys ['Accuracy','NMI','ARI'] and values in [0,1],\n",
    "    plot a bar chart showing these three metrics.\n",
    "    \"\"\"\n",
    "    labels = list(metrics_dict.keys())\n",
    "    values = [metrics_dict[k] for k in labels]\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    bars = plt.bar(labels, values, alpha=0.8, edgecolor='black', color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(title)\n",
    "    for bar, v in zip(bars, values):\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, v + 0.02, f\"{v:.2f}\", ha='center', va='bottom')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_intensity_histogram_with_centroids(image: np.ndarray, title: str, ax: plt.Axes):\n",
    "    \"\"\"\n",
    "    Plot histogram of pixel intensities (0..255) of the given image,\n",
    "    and overlay a dashed vertical line at the mean intensity (centroid).\n",
    "    \"\"\"\n",
    "    if len(image.shape) == 3: # If BGR, convert to grayscale for histogram\n",
    "        image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        image_gray = image\n",
    "\n",
    "    # Calculate histogram\n",
    "    hist = cv2.calcHist([image_gray], [0], None, [256], [0, 256])\n",
    "    ax.plot(hist, color='black')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Pixel Intensity')\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "    # Calculate and plot centroid (mean intensity value)\n",
    "    flat_image = image_gray.flatten()\n",
    "    if len(flat_image) > 0:\n",
    "        centroid = np.mean(flat_image)\n",
    "        ax.axvline(centroid, color='red', linestyle='dashed', linewidth=1)\n",
    "        ax.text(centroid + 5, ax.get_ylim()[1] * 0.9, f'Centroid: {centroid:.2f}', color='red')\n",
    "\n",
    "\n",
    "def show_results(orig: np.ndarray, clahe_img: np.ndarray,\n",
    "                 dbscan_segmented: np.ndarray, final_enhanced: np.ndarray,\n",
    "                 image_title: str):\n",
    "    \"\"\"\n",
    "    Display four images side by side with titles:\n",
    "    [Original | CLAHE Preprocessed | DBSCAN Segmented | Final Enhanced].\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    titles = [f'Original\\n({image_title})', 'CLAHE Preprocessed', 'DBSCAN Segmented', 'Final Enhanced']\n",
    "    # Ensure all images are BGR for consistent imshow display (Matplotlib expects RGB)\n",
    "    orig_rgb = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n",
    "    clahe_rgb = cv2.cvtColor(clahe_img, cv2.COLOR_GRAY2RGB) # Convert grayscale to RGB for display\n",
    "    dbscan_rgb = cv2.cvtColor(dbscan_segmented, cv2.COLOR_BGR2RGB) # Already BGR from segment_image_dbscan\n",
    "    final_rgb = cv2.cvtColor(final_enhanced, cv2.COLOR_GRAY2RGB) # Convert final enhanced (grayscale) to RGB\n",
    "    \n",
    "    imgs   = [orig_rgb, clahe_rgb, dbscan_rgb, final_rgb]\n",
    "\n",
    "    for ax, img, title in zip(axes, imgs, titles):\n",
    "        ax.imshow(img) # Matplotlib expects RGB\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Main Execution Block for Jupyter Notebook (Local) ---\n",
    "\n",
    "print(\"Welcome! This script will process images from a zip file using DBSCAN.\")\n",
    "zip_file_path = input(\"Please enter the full path to your zip file (e.g., C:\\\\Users\\\\YourName\\\\images.zip): \").strip()\n",
    "\n",
    "# Handle potential quotes from drag-and-drop on Windows\n",
    "if zip_file_path.startswith(\"'\") and zip_file_path.endswith(\"'\"):\n",
    "    zip_file_path = zip_file_path[1:-1]\n",
    "elif zip_file_path.startswith('\"') and zip_file_path.endswith('\"'):\n",
    "    zip_file_path = zip_file_path[1:-1]\n",
    "\n",
    "processed_image_data = [] # Stores (original_bgr, final_enhanced_gray, preprocessed_gray, dbscan_segmented_bgr, dbscan_labels_flat)\n",
    "image_names = []\n",
    "all_metrics_dict = {} # Stores metrics for each image: {'Image1': {'Acc': ..., 'NMI': ..., 'ARI': ...}, ...}\n",
    "\n",
    "# Create a directory to save enhanced images\n",
    "output_dir = \"enhanced_dbscan_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Important: Placeholder for Ground Truth ---\n",
    "# For demonstration, we'll assume the original grayscale image itself is a simple \"mask\"\n",
    "# or that you will provide a specific ground truth image within the zip for comparison.\n",
    "# If you have actual masked training images, place them in the zip and adjust the loading logic.\n",
    "# For now, we'll use a simplified assumption for comparison if a GT is not explicitly found.\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(zip_file_path):\n",
    "        raise FileNotFoundError(f\"Zip file not found at: {zip_file_path}\")\n",
    "\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zf:\n",
    "        image_files = [f for f in zf.namelist() if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff'))]\n",
    "        \n",
    "        if not image_files:\n",
    "            print(\"No image files found in the zip archive.\")\n",
    "        else:\n",
    "            print(f\"Found {len(image_files)} image(s) in the zip file.\")\n",
    "            \n",
    "            # Process up to 5 images\n",
    "            images_to_process = image_files[:5]\n",
    "            if len(image_files) > 5:\n",
    "                print(f\"Note: Processing only the first {len(images_to_process)} images found in the zip file.\")\n",
    "\n",
    "            for img_name in images_to_process:\n",
    "                print(f\"\\nProcessing {img_name}...\")\n",
    "                with zf.open(img_name) as img_file:\n",
    "                    img_data = img_file.read()\n",
    "                    img_array = np.frombuffer(img_data, np.uint8)\n",
    "                    \n",
    "                    img_raw = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
    "\n",
    "                    if img_raw is None:\n",
    "                        print(f\"Warning: Could not decode image {img_name}. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    # --- DBSCAN specific processing steps ---\n",
    "                    original_bgr, original_gray, preprocessed_gray = load_and_preprocess_image(img_raw)\n",
    "                    \n",
    "                    if preprocessed_gray is None:\n",
    "                        print(f\"Skipping {img_name} due to preprocessing error.\")\n",
    "                        continue\n",
    "\n",
    "                    # DBSCAN parameters (adjust these based on your image characteristics!)\n",
    "                    # eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "                    # min_samples: The number of samples (or total weight) in a neighborhood for a point to be considered as a core point.\n",
    "                    dbscan_labels, dbscan_segmented_bgr = segment_image_dbscan(preprocessed_gray, eps=5, min_samples=38) # Default values from your previous code\n",
    "                    \n",
    "                    # Blend the preprocessed image with the segmented result\n",
    "                    blended_image_gray = blend_images(preprocessed_gray, dbscan_segmented_bgr, alpha=0.7)\n",
    "                    \n",
    "                    # Apply final edge enhancement\n",
    "                    final_enhanced_image_gray = enhance_edges(blended_image_gray)\n",
    "                    \n",
    "                    processed_image_data.append((original_bgr, final_enhanced_image_gray, original_gray, dbscan_segmented_bgr, dbscan_labels))\n",
    "                    image_names.append(img_name)\n",
    "                    \n",
    "                    # --- Metrics Calculation (requires Ground Truth) ---\n",
    "                    # IMPORTANT: For real metric comparison, you need actual ground truth masks.\n",
    "                    # As a placeholder, we will use the preprocessed image as a pseudo ground truth\n",
    "                    # or assume the GT mask is in the zip with a specific naming convention (e.g., '_mask.png').\n",
    "                    # For this example, we'll simulate a simple 2-label GT based on intensity.\n",
    "                    # If you have specific GT masks, please provide them in the zip!\n",
    "                    \n",
    "                    # Pseudo Ground Truth (replace with actual GT loading if available)\n",
    "                    # For a simple binary GT, you might threshold the original image\n",
    "                    # Here, let's create a simple binary \"mask\" from preprocessed_gray\n",
    "                    # by just thresholding it. This is NOT a real GT, but allows the metrics functions to run.\n",
    "                    # You'd load a dedicated `gt_mask = cv2.imread(gt_mask_path, cv2.IMREAD_GRAYSCALE)` for real analysis.\n",
    "                    _, pseudo_gt_mask = cv2.threshold(preprocessed_gray, np.mean(preprocessed_gray), 1, cv2.THRESH_BINARY)\n",
    "                    pseudo_gt_mask = pseudo_gt_mask.astype(np.int32) # Ensure integer labels\n",
    "                    \n",
    "                    # Compute metrics\n",
    "                    # n_unique_pred_labels must include -1 if it's in dbscan_labels\n",
    "                    acc, nmi, ari = compute_segmentation_metrics(\n",
    "                        gt_mask=pseudo_gt_mask, \n",
    "                        pred_labels=dbscan_labels, \n",
    "                        n_unique_pred_labels=len(np.unique(dbscan_labels))\n",
    "                    )\n",
    "                    \n",
    "                    current_metrics = {'Accuracy': acc, 'NMI': nmi, 'ARI': ari}\n",
    "                    all_metrics_dict[img_name] = current_metrics\n",
    "                    print(f\"  Metrics for {img_name}: Accuracy={acc:.4f}, NMI={nmi:.4f}, ARI={ari:.4f}\")\n",
    "                    \n",
    "                    # Save the enhanced image\n",
    "                    base_name = os.path.splitext(os.path.basename(img_name))[0]\n",
    "                    output_path = os.path.join(output_dir, f\"enhanced_dbscan_{base_name}.png\")\n",
    "                    cv2.imwrite(output_path, cv2.cvtColor(final_enhanced_image_gray, cv2.COLOR_GRAY2BGR)) # Save as BGR\n",
    "                    print(f\"Enhanced image saved to {output_path}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: '{zip_file_path}' is not a valid zip file. Please check your file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# --- Display Results in Jupyter Notebook ---\n",
    "\n",
    "if not processed_image_data:\n",
    "    print(\"No images were successfully processed to display results.\")\n",
    "else:\n",
    "    # 1. Display all enhanced images (Original vs. CLAHE vs. DBSCAN Segmented vs. Final Enhanced)\n",
    "    print(\"\\n--- Original vs. DBSCAN Enhanced Images ---\")\n",
    "    for i, (original_bgr, final_enhanced_gray, preprocessed_gray, dbscan_segmented_bgr, _) in enumerate(processed_image_data):\n",
    "        show_results(original_bgr, preprocessed_gray, dbscan_segmented_bgr, final_enhanced_gray, os.path.basename(image_names[i]))\n",
    "\n",
    "    # 2. Bar chart of Segmentation Metrics for each image\n",
    "    print(\"\\n--- Segmentation Metrics for Each Image (DBSCAN) ---\")\n",
    "    for img_name, metrics in all_metrics_dict.items():\n",
    "        print(f\"\\nMetrics for {os.path.basename(img_name)}:\")\n",
    "        plot_metric_barchart(metrics, title=f'DBSCAN Metrics for {os.path.basename(img_name)}')\n",
    "\n",
    "    # 3. Histograms with Centroids for Original Grayscale and Final Enhanced Images\n",
    "    print(\"\\n--- Histograms with Centroids (Original vs. Final DBSCAN Enhanced) ---\")\n",
    "    num_images = len(processed_image_data)\n",
    "    fig_histograms, axes_hist = plt.subplots(num_images, 2, figsize=(15, 5 * num_images))\n",
    "    \n",
    "    if num_images == 1:\n",
    "        axes_hist = np.array([axes_hist]) # Ensure axes_hist is 2D for consistent indexing\n",
    "\n",
    "    for i, (original_bgr, final_enhanced_gray, original_gray, _, _) in enumerate(processed_image_data):\n",
    "        # Histogram for Original Grayscale Image\n",
    "        plot_histogram_with_centroids(original_gray, f'Original Histogram: {os.path.basename(image_names[i])}', axes_hist[i, 0])\n",
    "        \n",
    "        # Histogram for Final Enhanced Image (after DBSCAN, blending, and sharpening)\n",
    "        plot_histogram_with_centroids(final_enhanced_image_gray, f'DBSCAN Final Enhanced Histogram: {os.path.basename(image_names[i])}', axes_hist[i, 1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b47c067-754e-402e-b63f-80897c345f73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
